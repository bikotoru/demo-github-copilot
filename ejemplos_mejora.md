# üöÄ Ejemplos de Mejoras para AI Document Reader

Esta lista contiene prompts espec√≠ficos y claros para implementar mejoras en la aplicaci√≥n. Cada mejora est√° dise√±ada para ser implementada de forma independiente y probar las capacidades de IA para desarrollo.

## üìã Formato de Prompts

Cada prompt sigue esta estructura para mejores resultados:

```
#file:[nombre del archivo]
En [lenguaje de programaci√≥n], usando [framework], crea [descripci√≥n de la tarea]:
- Objetivo principal: [qu√© se quiere lograr]
- Requisitos espec√≠ficos: [lista detallada de caracter√≠sticas]
- Contexto: [situaci√≥n actual y por qu√© se necesita]
- Restricciones: [limitaciones t√©cnicas y de dise√±o]
```

## üéØ Mejoras Prioritarias

Las siguientes 3 mejoras est√°n listas para implementar con prompts estructurados:

---

# Mejora 1: Historial de Conversaciones Persistente

**Prompt Estructurado:**

```
#file:backend/main.py
#file:frontend/components/chat.tsx
#file:frontend/app/page.tsx

En Python usando FastAPI y TypeScript usando Next.js con React, crea un sistema de historial de conversaciones persistente:

- Objetivo principal: Guardar y recuperar conversaciones completas entre el usuario y la IA
- Requisitos espec√≠ficos:
  * Backend: Crear endpoints GET /conversations, GET /conversations/{id}, POST /conversations, DELETE /conversations/{id}
  * Cada conversaci√≥n debe tener: id √∫nico (UUID), t√≠tulo auto-generado, timestamp de creaci√≥n, array de mensajes
  * Almacenar datos en archivo JSON (conversations.json) sin base de datos
  * Frontend: Modificar el componente Chat existente para agregar sidebar izquierdo con lista de conversaciones
  * Implementar funcionalidad para crear nueva conversaci√≥n, cargar conversaci√≥n existente, eliminar conversaci√≥n
  * Auto-generar t√≠tulos basados en los primeros 50 caracteres del primer mensaje del usuario
  * Usar componentes ShadCN UI existentes (Button, ScrollArea, Separator, etc.)
- Contexto: La aplicaci√≥n actual usa el componente Chat en frontend/components/chat.tsx con streaming hacia backend FastAPI, pero las conversaciones se pierden al recargar
- Restricciones: 
  * No usar base de datos, solo archivos JSON
  * Mantener compatibilidad con el sistema de streaming actual en /chat/stream
  * El sidebar debe ser responsive y colapsable en m√≥viles usando componentes ShadCN UI
  * Mantener el theme provider y modo oscuro existente
  * M√°ximo 100 conversaciones guardadas (eliminar las m√°s antiguas autom√°ticamente)
```

**Archivos a modificar:**
- `backend/main.py` (nuevos endpoints)
- `frontend/components/chat.tsx` (sidebar y funcionalidad)
- Crear: `backend/conversations.json` (almacenamiento)

---

# Mejora 2: Soporte para Renderizado de Markdown

**Prompt Estructurado:**

```
#file:frontend/components/chat.tsx
#file:frontend/package.json

En TypeScript usando Next.js con React y ShadCN UI, implementa renderizado de Markdown en los mensajes del chat:

- Objetivo principal: Mostrar mensajes con formato Markdown (negrita, cursiva, c√≥digo, listas, enlaces, etc.)
- Requisitos espec√≠ficos:
  * Instalar y configurar react-markdown y react-syntax-highlighter
  * Modificar el componente Chat existente para renderizar mensajes de la IA con formato Markdown
  * Syntax highlighting para bloques de c√≥digo con nombres de lenguajes
  * Soporte para tablas, listas, enlaces, im√°genes
  * Mantener el estilo visual actual del chat (colores ShadCN UI, espaciado, bordes redondeados)
  * Agregar bot√≥n "copiar c√≥digo" en bloques de c√≥digo usando componentes ShadCN UI
  * Integrar con el tema oscuro/claro existente usando next-themes
- Contexto: La aplicaci√≥n actual usa frontend/components/chat.tsx que muestra mensajes como texto plano, pero la IA Gemini responde con Markdown
- Restricciones:
  * Mantener compatibilidad con el streaming actual (renderizar Markdown incremental durante el streaming)
  * Los estilos deben respetar el tema oscuro/claro actual usando CSS variables de ShadCN
  * Sanitizar contenido para evitar XSS
  * No afectar el rendimiento del streaming en /chat/stream
  * Usar solo los componentes ShadCN UI ya instalados donde sea posible
```

**Archivos a modificar:**
- `frontend/components/chat.tsx` (renderizado de markdown)
- `frontend/package.json` (nuevas dependencias)

---

# Mejora 3: Contador de Tokens de Entrada y Salida

**Prompt Estructurado:**

```
#file:backend/main.py
#file:frontend/components/chat.tsx

En Python usando FastAPI y TypeScript usando Next.js con ShadCN UI, implementa un sistema de conteo de tokens:

- Objetivo principal: Mostrar tokens de entrada y salida consumidos en cada mensaje para control de costos
- Requisitos espec√≠ficos:
  * Backend: Modificar el endpoint /chat/stream para calcular tokens antes y despu√©s de cada llamada a Gemini
  * Usar la funci√≥n count_tokens() de la API de Gemini para conteo preciso
  * Retornar en la respuesta streaming: input_tokens, output_tokens, total_tokens
  * Frontend: Modificar el componente Chat existente para mostrar contadores debajo de cada mensaje con iconos de Lucide React
  * Agregar contador total de la sesi√≥n en la cabecera del chat (junto al toggle de tema)
  * Mostrar costo estimado basado en precios actuales de Gemini (input: $0.000125/1K tokens, output: $0.000375/1K tokens)
  * Usar Badge component de ShadCN UI para mostrar la informaci√≥n de tokens
- Contexto: La aplicaci√≥n actual usa streaming en /chat/stream y el componente Chat, los usuarios necesitan controlar costos
- Restricciones:
  * Mantener compatibilidad con streaming actual (actualizar contadores en tiempo real durante el stream)
  * Los contadores deben ser discretos visualmente usando componentes ShadCN UI pero informativos
  * Incluir Toggle component para ocultar/mostrar informaci√≥n de tokens
  * Calcular tokens solo cuando sea necesario para optimizar rendimiento
  * Respetar el tema oscuro/claro existente
```

**Archivos a modificar:**
- `backend/main.py` (c√°lculo de tokens)
- `frontend/components/chat.tsx` (UI de contadores)

---

## üí° Instrucciones de Uso

1. **Copia el prompt estructurado** de cualquiera de las 3 mejoras anteriores
2. **P√©galo directamente** en tu modelo de IA preferido (Claude, ChatGPT, etc.)
3. **El modelo tendr√° contexto completo** sobre los archivos existentes y qu√© hacer
4. **Cada mejora es independiente** - puede implementarse sin las otras
5. **Formato probado** - esta estructura da mejores resultados que prompts simples

## üîÑ Pr√≥ximas Mejoras

Si estas 3 mejoras funcionan bien, podemos crear m√°s prompts estructurados para:
- Subida y an√°lisis de documentos
- Sistema de plantillas de prompts  
- An√°lisis de im√°genes con Gemini Vision
- Exportaci√≥n de conversaciones
- Dashboard de estad√≠sticas
- B√∫squeda global en conversaciones

---

# Mejora 4: Migraci√≥n del Backend a .NET 9

**Prompt Estructurado:**

```
#file:backend/main.py

En C# usando .NET 9 con ASP.NET Core Minimal APIs, migra completamente el backend actual de Python FastAPI a .NET:

- Objetivo principal: Reemplazar el backend Python por una implementaci√≥n equivalente en .NET 9 manteniendo toda la funcionalidad
- Requisitos espec√≠ficos:
  * Crear nuevo proyecto .NET 9 con Minimal APIs (Program.cs)
  * Implementar todos los endpoints existentes: GET /, GET /health, GET /test-gemini, POST /chat, POST /chat/stream
  * Integrar Google Gemini API usando HttpClient y JSON para las llamadas REST
  * Configurar CORS para permitir conexiones desde localhost:3000
  * Implementar streaming de respuestas usando Server-Sent Events (SSE)
  * Usar System.Text.Json para serializaci√≥n/deserializaci√≥n
  * Configurar variables de entorno con appsettings.json y User Secrets
  * Mantener la misma estructura de request/response que el Python actual
- Contexto: El backend Python funciona perfectamente, pero necesitamos una versi√≥n .NET para comparar rendimiento y explorar el ecosistema .NET
- Restricciones:
  * Mantener exactamente la misma API (mismos endpoints, mismos formatos JSON)
  * El frontend Next.js NO debe necesitar cambios
  * Usar solo librer√≠as nativas de .NET, no paquetes third-party para Gemini
  * Configurar puerto 8001 para evitar conflictos con el Python (puerto 8000)
  * Incluir logging con ILogger para debugging
  * Manejar errores de forma robusta con try-catch
```

**Archivos a crear:**
- `backend-dotnet/Program.cs` (API principal)
- `backend-dotnet/Models/ChatModels.cs` (DTOs)
- `backend-dotnet/Services/GeminiService.cs` (integraci√≥n con Gemini)
- `backend-dotnet/backend-dotnet.csproj` (proyecto)
- `backend-dotnet/appsettings.json` (configuraci√≥n)
- `backend-dotnet/README.md` (instrucciones espec√≠ficas .NET)

**Estructura esperada:**
```
backend-dotnet/
‚îú‚îÄ‚îÄ Program.cs
‚îú‚îÄ‚îÄ Models/
‚îÇ   ‚îî‚îÄ‚îÄ ChatModels.cs
‚îú‚îÄ‚îÄ Services/
‚îÇ   ‚îî‚îÄ‚îÄ GeminiService.cs
‚îú‚îÄ‚îÄ backend-dotnet.csproj
‚îú‚îÄ‚îÄ appsettings.json
‚îî‚îÄ‚îÄ README.md
```

---

**¬øListo para probar? Copia cualquier prompt y pru√©balo con tu IA favorita! üöÄ**